{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Word2Vec Exploration\n\nInterpreting and analyzing the trained embeddings. The testing here is mostly\nexploratory, so I only tried a few different hyperparameter configurations."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:47.475980Z",
     "start_time": "2026-02-21T11:19:47.434205200Z"
    }
   },
   "source": "import os\n\nimport numpy as np\nfrom word2vec.data import TEXT_PATH, load_tokens, build_vocab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Load embeddings and rebuild vocabulary"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:50.943544700Z",
     "start_time": "2026-02-21T11:19:47.480503400Z"
    }
   },
   "source": "tokens = load_tokens(TEXT_PATH)\nword_to_id, id_to_word, word_counts = build_vocab(tokens, min_count=5)\n\nW = np.load(\"embeddings/dim100_neg5_ep5.npy\")\nnorms = np.maximum(np.linalg.norm(W, axis=1, keepdims=True), 1e-12)\nW_norm = W / norms\nprint(f\"Embeddings: {W.shape}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 71290 words (min_count=5)\n",
      "Embeddings: (71290, 100)\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.010992500Z",
     "start_time": "2026-02-21T11:19:51.001155100Z"
    }
   },
   "source": "def nearest(word, top_n):\n    if word not in word_to_id:\n        return f\"'{word}' not in vocabulary\"\n    wid = word_to_id[word]\n    cos_sims = W_norm @ W_norm[wid]\n    cos_sims[wid] = -1\n    top_ids = np.argsort(-cos_sims)[:top_n]\n    return [(id_to_word[i], round(float(cos_sims[i]), 3)) for i in top_ids]\n\n\ndef analogy(a, b, c, top_n):\n    for w in [a, b, c]:\n        if w not in word_to_id:\n            return f\"'{w}' not in vocabulary\"\n    query = W_norm[word_to_id[b]] - W_norm[word_to_id[a]] + W_norm[word_to_id[c]]\n    query /= np.maximum(np.linalg.norm(query), 1e-12)\n    cos_sims = W_norm @ query\n    for w in [a, b, c]:\n        cos_sims[word_to_id[w]] = -1\n    top_ids = np.argsort(-cos_sims)[:top_n]\n    return [(id_to_word[i], round(float(cos_sims[i]), 3)) for i in top_ids]",
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "The model only learns from which words tend to appear near each other. I wanted to\n",
    "check whether that's enough to pick up on different kinds of similarity."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.041114300Z",
     "start_time": "2026-02-21T11:19:51.010992500Z"
    }
   },
   "source": [
    "for word in [\"university\", \"hospital\", \"war\", \"guitar\", \"ocean\"]:\n",
    "    print(f\"\\n{word}: {nearest(word, 5)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "university: [('polytechnic', 0.801), ('tsinghua', 0.778), ('gyeonggi', 0.768), ('throop', 0.765), ('ume', 0.764)]\n",
      "\n",
      "hospital: [('clinic', 0.82), ('nurse', 0.777), ('nursing', 0.757), ('hospitalized', 0.753), ('hospitals', 0.746)]\n",
      "\n",
      "war: [('wartime', 0.729), ('escalated', 0.714), ('bloodiest', 0.704), ('raged', 0.7), ('surrender', 0.7)]\n",
      "\n",
      "guitar: [('guitars', 0.89), ('bass', 0.887), ('drums', 0.869), ('bassists', 0.856), ('acoustic', 0.843)]\n",
      "\n",
      "ocean: [('atlantic', 0.832), ('strait', 0.805), ('volcanoes', 0.793), ('reefs', 0.789), ('atolls', 0.785)]\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.083920800Z",
     "start_time": "2026-02-21T11:19:51.050896500Z"
    }
   },
   "source": [
    "for word in [\"three\", \"hundred\", \"million\"]:\n",
    "    print(f\"{word}: {nearest(word, 5)} \\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "three: [('four', 0.785), ('seven', 0.712), ('six', 0.696), ('two', 0.672), ('five', 0.664)] \n",
      "\n",
      "hundred: [('thousand', 0.835), ('forty', 0.749), ('fifty', 0.744), ('twenty', 0.734), ('sixty', 0.73)] \n",
      "\n",
      "million: [('estimated', 0.77), ('billion', 0.741), ('totaled', 0.706), ('approximately', 0.703), ('rmb', 0.693)] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy tests\n",
    "\n",
    "a : b :: c : ? solved via vec(b) - vec(a) + vec(c).\n",
    "\n",
    "If the model learns consistent directions for relationships (e.g. a \"gender\" direction,\n",
    "a \"capital-of\" direction) then vector arithmetic should recover them."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.161068200Z",
     "start_time": "2026-02-21T11:19:51.095822100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tests = [\n",
    "    (\"king\",          \"man\",       \"queen\",     \"woman\"),\n",
    "    (\"madrid\",        \"spain\",     \"berlin\",    \"germany\"),\n",
    "    (\"france\",        \"latin\",     \"poland\",    \"slavic\"),\n",
    "    (\"man\",           \"woman\",     \"uncle\",     \"aunt\"),\n",
    "    (\"going\",         \"went\",      \"playing\",   \"played\"),\n",
    "    (\"brother\",       \"sister\",    \"father\",    \"mother\"),\n",
    "    (\"car\",           \"cars\",      \"dog\",       \"dogs\"),\n",
    "    (\"doctor\",        \"medicine\",  \"professor\", \"science\"),\n",
    "    (\"christianity\",  \"jesus\",     \"islam\",     \"muhammad\"),\n",
    "    (\"italy\",         \"europe\",    \"japan\",     \"asia\"),\n",
    "]\n",
    "\n",
    "for a, b, c, _ in tests:\n",
    "    results = analogy(a, b, c, 3)\n",
    "    print(f\"{a} : {b} :: {c} : ? -> {results} \\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king : man :: queen : ? -> [('sally', 0.512), ('woman', 0.51), ('cobbler', 0.506)] \n",
      "\n",
      "madrid : spain :: berlin : ? -> [('germany', 0.639), ('austria', 0.606), ('vienna', 0.558)] \n",
      "\n",
      "france : latin :: poland : ? -> [('slavic', 0.645), ('alphabet', 0.579), ('runic', 0.579)] \n",
      "\n",
      "man : woman :: uncle : ? -> [('aunt', 0.685), ('grandmother', 0.683), ('mother', 0.674)] \n",
      "\n",
      "going : went :: playing : ? -> [('played', 0.611), ('virtuoso', 0.599), ('toured', 0.592)] \n",
      "\n",
      "brother : sister :: father : ? -> [('daughter', 0.644), ('mother', 0.592), ('sisters', 0.578)] \n",
      "\n",
      "car : cars :: dog : ? -> [('dogs', 0.714), ('breeds', 0.666), ('keeshond', 0.664)] \n",
      "\n",
      "doctor : medicine :: professor : ? -> [('phd', 0.699), ('sciences', 0.686), ('integrative', 0.657)] \n",
      "\n",
      "christianity : jesus :: islam : ? -> [('allah', 0.796), ('muhammad', 0.749), ('prophet', 0.748)] \n",
      "\n",
      "italy : europe :: japan : ? -> [('asia', 0.617), ('china', 0.565), ('thailand', 0.558)] \n",
      "\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From what I could tell the dataset is heavy on history and geography (makes sense\nsince it's from Wikipedia), so that's the kind of relationships I was mostly after\nwhen picking the test cases. 6 out of 10 got the expected word as the top-1 result\n(madrid/berlin, france/slavic, uncle/aunt, going/played, car/dogs, italy/asia).\nA few near misses: king/queen gives \"woman\" as the second result,\nbrother/sister::father gives \"daughter\" instead of \"mother\", and\nchristianity/jesus::islam gives \"allah\" first but \"muhammad\" second."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Hyperparameter comparison\n\nI tested four different hyperparameter configurations, as I was interested how their\nresults would compare on these kinds of tests. Each varies one parameter from the baseline.\n\n| Config | dim | neg | epochs | train time |\n|--------|-----|-----|--------|------------|\n| baseline | 100 | 5 | 5 | ~25 min |\n| more negatives | 100 | 15 | 5 | ~73 min |\n| higher dim | 200 | 5 | 5 | ~55 min |\n| fewer epochs | 100 | 5 | 1 | ~5 min |"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.342170900Z",
     "start_time": "2026-02-21T11:19:51.161068200Z"
    }
   },
   "source": [
    "def normalise_rows(M):\n",
    "    return M / np.maximum(np.linalg.norm(M, axis=1, keepdims=True), 1e-12)\n",
    "\n",
    "EMBED_DIR = \"embeddings\"\n",
    "\n",
    "configs = {\n",
    "    \"dim100_neg5_ep5\":  \"baseline (d=100, neg=5, ep=5)\",\n",
    "    \"dim100_neg15_ep5\": \"more negatives (neg=15)\",\n",
    "    \"dim200_neg5_ep5\":  \"higher dim (d=200)\",\n",
    "    \"dim100_neg5_ep1\":  \"fewer epochs (ep=1)\",\n",
    "}\n",
    "\n",
    "embeddings = {}\n",
    "for key, label in configs.items():\n",
    "    path = os.path.join(EMBED_DIR, f\"{key}.npy\")\n",
    "    E = np.load(path)\n",
    "    embeddings[key] = normalise_rows(E)"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:19:51.516148900Z",
     "start_time": "2026-02-21T11:19:51.342170900Z"
    }
   },
   "source": "def analogy_with(normed, a, b, c):\n    for w in [a, b, c]:\n        if w not in word_to_id:\n            return None\n    query = normed[word_to_id[b]] - normed[word_to_id[a]] + normed[word_to_id[c]]\n    query /= np.maximum(np.linalg.norm(query), 1e-12)\n    cos_sims = normed @ query\n    for w in [a, b, c]:\n        cos_sims[word_to_id[w]] = -1\n    return id_to_word[np.argmax(cos_sims)]\n\n\ntrain_times = {\n    \"dim100_neg5_ep5\":  25.5,\n    \"dim100_neg15_ep5\": 72.8,\n    \"dim200_neg5_ep5\":  55.0,\n    \"dim100_neg5_ep1\":   5.4,\n}\n\nprint(f\"{'Config':<30} {'Correct':>7} {'Accuracy':>8} {'Time':>10}\")\nprint(\"-\" * 58)\nfor key in configs:\n    if key not in embeddings:\n        continue\n    normed = embeddings[key]\n    correct = sum(1 for a, b, c, exp in tests if analogy_with(normed, a, b, c) == exp)\n    total = sum(1 for a, b, c, exp in tests if analogy_with(normed, a, b, c) is not None)\n    acc = correct / total if total > 0 else 0\n    t = train_times.get(key, 0)\n    print(f\"{configs[key]:<30} {correct:>3}/{total:<3} {acc:>8.1%} {t:>7.1f} min\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config                         Correct Accuracy       Time\n",
      "----------------------------------------------------------\n",
      "baseline (d=100, neg=5, ep=5)    6/10     60.0%    25.5 min\n",
      "more negatives (neg=15)          6/10     60.0%    72.8 min\n",
      "higher dim (d=200)               7/10     70.0%    55.0 min\n",
      "fewer epochs (ep=1)              2/10     20.0%     5.4 min\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Side-by-side nearest neighbors"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-21T11:20:02.119327300Z",
     "start_time": "2026-02-21T11:20:02.000871200Z"
    }
   },
   "source": [
    "def nearest_with(normed, word, top_n):\n",
    "    if word not in word_to_id:\n",
    "        return []\n",
    "    wid = word_to_id[word]\n",
    "    cos_sims = normed @ normed[wid]\n",
    "    cos_sims[wid] = -1\n",
    "    top_ids = np.argsort(-cos_sims)[:top_n]\n",
    "    return [id_to_word[i] for i in top_ids]\n",
    "\n",
    "\n",
    "for word in [\"university\", \"hospital\", \"war\", \"guitar\", \"ocean\"]:\n",
    "    print(f\"\\n'{word}':\")\n",
    "    for key in configs:\n",
    "        if key not in embeddings:\n",
    "            continue\n",
    "        neighbors = nearest_with(embeddings[key], word, 5)\n",
    "        print(f\"  {configs[key]}: {', '.join(neighbors)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'university':\n",
      "  baseline (d=100, neg=5, ep=5): polytechnic, tsinghua, gyeonggi, throop, ume\n",
      "  more negatives (neg=15): polytechnic, tsinghua, gyeonggi, ume, throop\n",
      "  higher dim (d=200): polytechnic, gyeonggi, throop, abet, ume\n",
      "  fewer epochs (ep=1): college, alumni, oxford, indiana, angeles\n",
      "\n",
      "'hospital':\n",
      "  baseline (d=100, neg=5, ep=5): clinic, nurse, nursing, hospitalized, hospitals\n",
      "  more negatives (neg=15): clinic, hospitalized, hospitals, nurse, pneumonia\n",
      "  higher dim (d=200): clinic, nurse, nursing, hospitals, hospitalized\n",
      "  fewer epochs (ep=1): fellow, illegitimate, commissioned, murdered, daughters\n",
      "\n",
      "'war':\n",
      "  baseline (d=100, neg=5, ep=5): wartime, escalated, bloodiest, raged, surrender\n",
      "  more negatives (neg=15): allied, escalated, sepoys, bloodiest, manchukuo\n",
      "  higher dim (d=200): escalated, bloodiest, invasion, wartime, manchukuo\n",
      "  fewer epochs (ep=1): communist, soviet, democratic, civil, britain\n",
      "\n",
      "'guitar':\n",
      "  baseline (d=100, neg=5, ep=5): guitars, bass, drums, bassists, acoustic\n",
      "  more negatives (neg=15): guitars, bass, drums, acoustic, bassists\n",
      "  higher dim (d=200): bass, guitars, drums, bassists, vocals\n",
      "  fewer epochs (ep=1): bass, guitars, percussion, faq, ftp\n",
      "\n",
      "'ocean':\n",
      "  baseline (d=100, neg=5, ep=5): atlantic, strait, volcanoes, reefs, atolls\n",
      "  more negatives (neg=15): atolls, westerly, mcmurdo, oceanic, volcanoes\n",
      "  higher dim (d=200): atlantic, volcanoes, aleutian, atolls, oceanic\n",
      "  fewer epochs (ep=1): sea, river, coastal, coast, maritime\n"
     ]
    }
   ],
   "execution_count": 86
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
